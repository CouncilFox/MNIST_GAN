{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Standard GAN\n",
    "## Overview\n",
    "Dataset: MNIST handwritten digits.\n",
    "Framework: TensorFlow 2.x with Keras API.\n",
    "GAN Components:\n",
    "- Generator: Transforms random noise into synthetic MNIST-like images.\n",
    "- Discriminator: Classifies images as real or fake.\n",
    "- Loss Function: Binary Cross-Entropy (Standard GAN loss).\n",
    "- Optimization: Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Preprocess MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize images to [0, 1] range and add a channel dimension\n",
    "train_images = train_images.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# Create batches and shuffle the dataset\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(train_images)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorde\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator():\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32, (5, 5), strides=(2, 2), padding=\"same\", input_shape=[28, 28, 1]\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorde\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_generator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(7 * 7 * 192, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 192)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.UpSampling2D())\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            96, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.UpSampling2D())\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            48, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            24, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            1,\n",
    "            (5, 5),\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "            activation=\"sigmoid\",\n",
    "        )\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(\n",
    "        tf.ones_like(real_output), real_output\n",
    "    )  # Labels for real images are 1\n",
    "    fake_loss = cross_entropy(\n",
    "        tf.zeros_like(fake_output), fake_output\n",
    "    )  # Labels for fake images are 0\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # Generator tries to make fake images be classified as real (label = 1)\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = optimizers.Adam(learning_rate=1e-4)\n",
    "discriminator_optimizer = optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Seed for generating images during training\n",
    "seed = tf.random.normal([16, 100])\n",
    "\n",
    "\n",
    "# Training function\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, 100])\n",
    "\n",
    "    # Record operations for automatic differentiation\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        # Discriminator outputs\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        # Calculate losses\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(\n",
    "        disc_loss, discriminator.trainable_variables\n",
    "    )\n",
    "\n",
    "    # Apply gradients to update the weights\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator, generator.trainable_variables)\n",
    "    )\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "    )\n",
    "\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "# Create a checkpoint object\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    ")\n",
    "\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Produce images for the GIF as we go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "        # Save the model every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "            generator.save(f\"generator_epoch_{epoch + 1}.h5\")\n",
    "            discriminator.save(f\"discriminator_epoch_{epoch + 1}.h5\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed)\n",
    "\n",
    "    # Save the final models\n",
    "    generator.save(\"generator_model.h5\")\n",
    "    discriminator.save(\"discriminator_model.h5\")\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Function to Generate and Save Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice training=False\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.savefig(f\"image_at_epoch_{epoch:04d}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "train(dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
